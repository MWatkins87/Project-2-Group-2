{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/45/6d/8c1d2570a52db6263d855c3ee3daf8f4bdf4a365cd6610772d6fce5fd904/xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata\n",
      "  Using cached xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /Users/beau/anaconda3/lib/python3.11/site-packages (from xgboost) (1.26.3)\n",
      "Requirement already satisfied: scipy in /Users/beau/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.4)\n",
      "Using cached xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.2 MB)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import joblib\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from xgboost import XGBClassifier\n",
    "    from scipy import stats\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.datasets import make_regression, make_swiss_roll\n",
    "    from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "    from sklearn.svm import SVC, SVR\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "    print('All libraries imported successfully!')\n",
    "except ImportError:\n",
    "    print('Some libraries failed to import.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tedsd_puf_2019.csv loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISYR</th>\n",
       "      <th>CASEID</th>\n",
       "      <th>STFIPS</th>\n",
       "      <th>CBSA2010</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>MARSTAT</th>\n",
       "      <th>SERVICES</th>\n",
       "      <th>DETCRIM</th>\n",
       "      <th>LOS</th>\n",
       "      <th>PSOURCE</th>\n",
       "      <th>...</th>\n",
       "      <th>TRNQFLG</th>\n",
       "      <th>BARBFLG</th>\n",
       "      <th>SEDHPFLG</th>\n",
       "      <th>INHFLG</th>\n",
       "      <th>OTCFLG</th>\n",
       "      <th>OTHERFLG</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>REGION</th>\n",
       "      <th>IDU</th>\n",
       "      <th>ALCDRUG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019</td>\n",
       "      <td>20191553576</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019</td>\n",
       "      <td>20191465214</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019</td>\n",
       "      <td>20191443889</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019</td>\n",
       "      <td>20191409377</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>-9</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019</td>\n",
       "      <td>20191479567</td>\n",
       "      <td>2</td>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>-9</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DISYR       CASEID  STFIPS  CBSA2010  EDUC  MARSTAT  SERVICES  DETCRIM  \\\n",
       "0   2019  20191553576       2        -9     4        1         7       -9   \n",
       "1   2019  20191465214       2        -9     3        1         7       -9   \n",
       "2   2019  20191443889       2        -9     2        1         7       -9   \n",
       "3   2019  20191409377       2        -9     3        1         7       -9   \n",
       "4   2019  20191479567       2        -9     3        3         7       -9   \n",
       "\n",
       "   LOS  PSOURCE  ...  TRNQFLG  BARBFLG  SEDHPFLG  INHFLG  OTCFLG  OTHERFLG  \\\n",
       "0   37        1  ...        0        0         0       0       0         0   \n",
       "1   35        1  ...        0        0         0       0       0         0   \n",
       "2   35        1  ...        0        0         0       0       0         0   \n",
       "3   37        1  ...        0        0         0       0       0         0   \n",
       "4   37        1  ...        0        0         0       0       0         0   \n",
       "\n",
       "   DIVISION  REGION  IDU  ALCDRUG  \n",
       "0         9       4    0        1  \n",
       "1         9       4    0        3  \n",
       "2         9       4    0        3  \n",
       "3         9       4    0        3  \n",
       "4         9       4    0        1  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from a CSV file\n",
    "df = pd.read_csv('tedsd_puf_2019.csv')\n",
    "print('tedsd_puf_2019.csv loaded')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1722503, 76)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DISYR</th>\n",
       "      <th>CASEID</th>\n",
       "      <th>STFIPS</th>\n",
       "      <th>CBSA2010</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>MARSTAT</th>\n",
       "      <th>SERVICES</th>\n",
       "      <th>DETCRIM</th>\n",
       "      <th>LOS</th>\n",
       "      <th>PSOURCE</th>\n",
       "      <th>...</th>\n",
       "      <th>TRNQFLG</th>\n",
       "      <th>BARBFLG</th>\n",
       "      <th>SEDHPFLG</th>\n",
       "      <th>INHFLG</th>\n",
       "      <th>OTCFLG</th>\n",
       "      <th>OTHERFLG</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>REGION</th>\n",
       "      <th>IDU</th>\n",
       "      <th>ALCDRUG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1722503.0</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "      <td>1.722503e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.019087e+10</td>\n",
       "      <td>2.558714e+01</td>\n",
       "      <td>1.817492e+04</td>\n",
       "      <td>1.777766e+00</td>\n",
       "      <td>-5.340136e-01</td>\n",
       "      <td>5.540790e+00</td>\n",
       "      <td>-6.617934e+00</td>\n",
       "      <td>2.018519e+01</td>\n",
       "      <td>2.483211e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.146584e-04</td>\n",
       "      <td>1.067052e-03</td>\n",
       "      <td>4.600863e-03</td>\n",
       "      <td>1.085629e-03</td>\n",
       "      <td>1.239475e-03</td>\n",
       "      <td>3.434421e-02</td>\n",
       "      <td>4.596869e+00</td>\n",
       "      <td>2.420380e+00</td>\n",
       "      <td>-4.622192e-01</td>\n",
       "      <td>1.945627e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.065654e+05</td>\n",
       "      <td>1.427222e+01</td>\n",
       "      <td>1.728295e+04</td>\n",
       "      <td>3.731331e+00</td>\n",
       "      <td>4.470499e+00</td>\n",
       "      <td>1.938862e+00</td>\n",
       "      <td>5.031978e+00</td>\n",
       "      <td>1.433813e+01</td>\n",
       "      <td>4.037795e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.773583e-02</td>\n",
       "      <td>3.264834e-02</td>\n",
       "      <td>6.767346e-02</td>\n",
       "      <td>3.293101e-02</td>\n",
       "      <td>3.518437e-02</td>\n",
       "      <td>1.821118e-01</td>\n",
       "      <td>2.560715e+00</td>\n",
       "      <td>1.144867e+00</td>\n",
       "      <td>2.492082e+00</td>\n",
       "      <td>8.534534e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.019000e+10</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.019044e+10</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.019087e+10</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>1.674000e+04</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>2.600000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.019131e+10</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>3.562000e+04</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>-9.000000e+00</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>2.019176e+10</td>\n",
       "      <td>7.200000e+01</td>\n",
       "      <td>4.970000e+04</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>7.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DISYR        CASEID        STFIPS      CBSA2010          EDUC  \\\n",
       "count  1722503.0  1.722503e+06  1.722503e+06  1.722503e+06  1.722503e+06   \n",
       "mean      2019.0  2.019087e+10  2.558714e+01  1.817492e+04  1.777766e+00   \n",
       "std          0.0  5.065654e+05  1.427222e+01  1.728295e+04  3.731331e+00   \n",
       "min       2019.0  2.019000e+10  1.000000e+00 -9.000000e+00 -9.000000e+00   \n",
       "25%       2019.0  2.019044e+10  1.200000e+01 -9.000000e+00  2.000000e+00   \n",
       "50%       2019.0  2.019087e+10  2.700000e+01  1.674000e+04  3.000000e+00   \n",
       "75%       2019.0  2.019131e+10  3.600000e+01  3.562000e+04  3.000000e+00   \n",
       "max       2019.0  2.019176e+10  7.200000e+01  4.970000e+04  5.000000e+00   \n",
       "\n",
       "            MARSTAT      SERVICES       DETCRIM           LOS       PSOURCE  \\\n",
       "count  1.722503e+06  1.722503e+06  1.722503e+06  1.722503e+06  1.722503e+06   \n",
       "mean  -5.340136e-01  5.540790e+00 -6.617934e+00  2.018519e+01  2.483211e+00   \n",
       "std    4.470499e+00  1.938862e+00  5.031978e+00  1.433813e+01  4.037795e+00   \n",
       "min   -9.000000e+00  1.000000e+00 -9.000000e+00  1.000000e+00 -9.000000e+00   \n",
       "25%    1.000000e+00  4.000000e+00 -9.000000e+00  4.000000e+00  1.000000e+00   \n",
       "50%    1.000000e+00  7.000000e+00 -9.000000e+00  2.600000e+01  2.000000e+00   \n",
       "75%    2.000000e+00  7.000000e+00 -9.000000e+00  3.400000e+01  6.000000e+00   \n",
       "max    4.000000e+00  8.000000e+00  8.000000e+00  3.700000e+01  7.000000e+00   \n",
       "\n",
       "       ...       TRNQFLG       BARBFLG      SEDHPFLG        INHFLG  \\\n",
       "count  ...  1.722503e+06  1.722503e+06  1.722503e+06  1.722503e+06   \n",
       "mean   ...  3.146584e-04  1.067052e-03  4.600863e-03  1.085629e-03   \n",
       "std    ...  1.773583e-02  3.264834e-02  6.767346e-02  3.293101e-02   \n",
       "min    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "50%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "75%    ...  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "max    ...  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \n",
       "\n",
       "             OTCFLG      OTHERFLG      DIVISION        REGION           IDU  \\\n",
       "count  1.722503e+06  1.722503e+06  1.722503e+06  1.722503e+06  1.722503e+06   \n",
       "mean   1.239475e-03  3.434421e-02  4.596869e+00  2.420380e+00 -4.622192e-01   \n",
       "std    3.518437e-02  1.821118e-01  2.560715e+00  1.144867e+00  2.492082e+00   \n",
       "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00 -9.000000e+00   \n",
       "25%    0.000000e+00  0.000000e+00  2.000000e+00  1.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00  0.000000e+00  5.000000e+00  3.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00  0.000000e+00  7.000000e+00  3.000000e+00  0.000000e+00   \n",
       "max    1.000000e+00  1.000000e+00  9.000000e+00  4.000000e+00  1.000000e+00   \n",
       "\n",
       "            ALCDRUG  \n",
       "count  1.722503e+06  \n",
       "mean   1.945627e+00  \n",
       "std    8.534534e-01  \n",
       "min    0.000000e+00  \n",
       "25%    2.000000e+00  \n",
       "50%    2.000000e+00  \n",
       "75%    3.000000e+00  \n",
       "max    3.000000e+00  \n",
       "\n",
       "[8 rows x 76 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1722503 entries, 0 to 1722502\n",
      "Data columns (total 76 columns):\n",
      " #   Column                 Dtype\n",
      "---  ------                 -----\n",
      " 0   DISYR                  int64\n",
      " 1   CASEID                 int64\n",
      " 2   STFIPS                 int64\n",
      " 3   CBSA2010               int64\n",
      " 4   EDUC                   int64\n",
      " 5   MARSTAT                int64\n",
      " 6   SERVICES               int64\n",
      " 7   DETCRIM                int64\n",
      " 8   LOS                    int64\n",
      " 9   PSOURCE                int64\n",
      " 10  NOPRIOR                int64\n",
      " 11  ARRESTS                int64\n",
      " 12  EMPLOY                 int64\n",
      " 13  METHUSE                int64\n",
      " 14  PSYPROB                int64\n",
      " 15  PREG                   int64\n",
      " 16  GENDER                 int64\n",
      " 17  VET                    int64\n",
      " 18  LIVARAG                int64\n",
      " 19  DAYWAIT                int64\n",
      " 20  SERVICES_D             int64\n",
      " 21  REASON                 int64\n",
      " 22  EMPLOY_D               int64\n",
      " 23  LIVARAG_D              int64\n",
      " 24  ARRESTS_D              int64\n",
      " 25  DSMCRIT                int64\n",
      " 26  AGE                    int64\n",
      " 27  RACE                   int64\n",
      " 28  ETHNIC                 int64\n",
      " 29  DETNLF                 int64\n",
      " 30  DETNLF_D               int64\n",
      " 31  PRIMINC                int64\n",
      " 32  SUB1                   int64\n",
      " 33  SUB2                   int64\n",
      " 34  SUB3                   int64\n",
      " 35  SUB1_D                 int64\n",
      " 36  SUB2_D                 int64\n",
      " 37  SUB3_D                 int64\n",
      " 38  ROUTE1                 int64\n",
      " 39  ROUTE2                 int64\n",
      " 40  ROUTE3                 int64\n",
      " 41  FREQ1                  int64\n",
      " 42  FREQ2                  int64\n",
      " 43  FREQ3                  int64\n",
      " 44  FREQ1_D                int64\n",
      " 45  FREQ2_D                int64\n",
      " 46  FREQ3_D                int64\n",
      " 47  FRSTUSE1               int64\n",
      " 48  FRSTUSE2               int64\n",
      " 49  FRSTUSE3               int64\n",
      " 50  HLTHINS                int64\n",
      " 51  PRIMPAY                int64\n",
      " 52  FREQ_ATND_SELF_HELP    int64\n",
      " 53  FREQ_ATND_SELF_HELP_D  int64\n",
      " 54  ALCFLG                 int64\n",
      " 55  COKEFLG                int64\n",
      " 56  MARFLG                 int64\n",
      " 57  HERFLG                 int64\n",
      " 58  METHFLG                int64\n",
      " 59  OPSYNFLG               int64\n",
      " 60  PCPFLG                 int64\n",
      " 61  HALLFLG                int64\n",
      " 62  MTHAMFLG               int64\n",
      " 63  AMPHFLG                int64\n",
      " 64  STIMFLG                int64\n",
      " 65  BENZFLG                int64\n",
      " 66  TRNQFLG                int64\n",
      " 67  BARBFLG                int64\n",
      " 68  SEDHPFLG               int64\n",
      " 69  INHFLG                 int64\n",
      " 70  OTCFLG                 int64\n",
      " 71  OTHERFLG               int64\n",
      " 72  DIVISION               int64\n",
      " 73  REGION                 int64\n",
      " 74  IDU                    int64\n",
      " 75  ALCDRUG                int64\n",
      "dtypes: int64(76)\n",
      "memory usage: 998.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize the data\n",
    "display(df.shape)\n",
    "display(df.describe())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a copy of the data\n",
    "# df1 = df.copy()\n",
    "# print('Copy of the DataFrame')\n",
    "# display(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In this dataset, -9 is used to represent missing values. Which columns have -9 as a value and how many?\n",
    "# missing_values = df1.isin([-9]).sum()\n",
    "# display(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What specific columns have -9 as a value?\n",
    "# missing_columns = missing_values[missing_values > 0]\n",
    "# display(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How many columns have -9 as a value?\n",
    "# display(len(missing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize all columns with -9 as a value\n",
    "# missing_columns.plot(kind='bar', figsize=(15, 7))\n",
    "# plt.title('Columns with -9 as a value')\n",
    "# plt.xlabel('Columns')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9 with NaN\n",
    "# df1 = df1.replace(-9, np.nan)\n",
    "# print('Replacing -9 with NaN. A -9 represents a missing value code in the dataset.')\n",
    "# display(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that contain NaN values\n",
    "# columns_with_nan = df1.isna().any()\n",
    "\n",
    "# # Count the number of columns that contain NaN values\n",
    "# num_columns_with_nan = columns_with_nan.sum()\n",
    "\n",
    "# # Print the number of columns that contain NaN values\n",
    "# print('There are',num_columns_with_nan,'columns that contain NaN.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that contain NaN values\n",
    "# columns_with_nan = df1.columns[df1.isna().any()].tolist()\n",
    "\n",
    "# # Calculate the percentage of NaN values in each of these columns\n",
    "# nan_percentage = df1[columns_with_nan].isna().mean() * 100\n",
    "\n",
    "# # Sort the percentages in ascending order\n",
    "# nan_percentage_sorted = nan_percentage.sort_values(ascending=False)\n",
    "\n",
    "# # Print the percentage of NaN values\n",
    "# print('Percentage of NaN values in each column.')\n",
    "# display(nan_percentage_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "# plt.figure(figsize=(20,18))\n",
    "# sns.barplot(x=nan_percentage_sorted.values, y=nan_percentage_sorted.index)\n",
    "# plt.title('Percentage of NaN values in columns')\n",
    "# # Show the plot\n",
    "# print('Bar plot showing the percentage of NaN values in columns.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reason = df1['REASON']\n",
    "# df1_corr = df1.corr()\n",
    "# df1_corr.unstack().sort_values()\n",
    "# variable = df1_corr['REASON'].sort_values()\n",
    "# print('Correlation of REASON with other variables.')\n",
    "# variable.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of colors based on the values in 'variable'\n",
    "# colors = ['red' if x < 0 else 'blue' for x in variable[:-1]]\n",
    "\n",
    "# # Visualize correlation to the target variable 'REASON'\n",
    "# plt.figure(figsize=(20,18))\n",
    "# variable[:-1].plot(kind='bar', color=colors)\n",
    "# plt.title('Correlation to the target variable')\n",
    "# print('Visualizing the positive and negative correlations of REASON with other variables.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only positive correlations\n",
    "# positive_correlations = variable[variable > 0]\n",
    "\n",
    "# # Visualize positive correlations\n",
    "# plt.figure(figsize=(20,18))\n",
    "# positive_correlations[:-1].plot(kind='bar', color='blue')\n",
    "# plt.title('This is a visualization of only the positive correlations to the target variable')\n",
    "# print('Positive correlations of REASON with other variables.')\n",
    "# print(\"\\nSERVICES is the highest positive correlation and EMPLOY is the lowest positive correlation at the moment.\")\n",
    "# print(\"\\nMore exploration is needed to understand the relationship between the target variable and the features.\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('These are the columns with NaN values, with the highest percentage of NaN values at the top.')\n",
    "# print(\"\\nFor example, the column FREQ3_D has roughly 83.66 precent of its values as NaN.\")\n",
    "# print('\\nThis means that 83.66 percent of the values in the column FREQ3_D are missing or not reported.')\n",
    "# display(nan_percentage_sorted)\n",
    "# display(df1_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column names from the first set\n",
    "# first_set = [\n",
    "#     \"FREQ3_D\", \"DETCRIM\", \"FREQ3\", \"FRSTUSE3\", \"ROUTE3\", \"DETNLF_D\", \"DETNLF\",\n",
    "#     \"FREQ2_D\", \"PREG\", \"PRIMPAY\", \"DAYWAIT\", \"HLTHINS\", \"FRSTUSE2\", \"FREQ2\",\n",
    "#     \"ROUTE2\", \"CBSA2010\", \"PRIMINC\", \"FREQ1_D\", \"DSMCRIT\", \"FREQ_ATND_SELF_HELP_D\",\n",
    "#     \"MARSTAT\", \"LIVARAG_D\", \"ARRESTS_D\", \"FREQ_ATND_SELF_HELP\", \"EMPLOY_D\", \"PSYPROB\",\n",
    "#     \"SUB1_D\", \"FREQ1\", \"SUB3\", \"ARRESTS\", \"METHUSE\", \"EDUC\", \"LIVARAG\", \"VET\", \"EMPLOY\",\n",
    "#     \"FRSTUSE1\", \"ROUTE1\", \"NOPRIOR\", \"IDU\", \"SUB2_D\", \"PSOURCE\", \"SUB1\", \"SUB3_D\",\n",
    "#     \"SUB2\", \"ETHNIC\", \"RACE\", \"GENDER\"\n",
    "# ]\n",
    "\n",
    "# Define the column names from the second set\n",
    "# second_set = [\n",
    "#     \"DISYR\", \"CASEID\", \"STFIPS\", \"CBSA2010\", \"EDUC\", \"MARSTAT\", \"SERVICES\",\n",
    "#     \"DETCRIM\", \"LOS\", \"PSOURCE\", \"TRNQFLG\", \"BARBFLG\", \"SEDHPFLG\", \"INHFLG\",\n",
    "#     \"OTCFLG\", \"OTHERFLG\", \"DIVISION\", \"REGION\", \"IDU\", \"ALCDRUG\"\n",
    "# ]\n",
    "\n",
    "# Find the common column names\n",
    "# common_columns = list(set(first_set).intersection(second_set))\n",
    "\n",
    "# # Add quotations around each column name\n",
    "# first_set_quoted = [f'\"{col}\"' for col in first_set]\n",
    "# second_set_quoted = [f'\"{col}\"' for col in second_set]\n",
    "# common_columns_quoted = [f'\"{col}\"' for col in common_columns]\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Looking for common columns in two sets of column names to see if they need be removed or filled.\")\n",
    "# print(\"\\nColumn Names from the First Set:(nan_percentage_sorted)\")\n",
    "# print(\", \".join(first_set_quoted))\n",
    "# print(\"\\nColumn Names from the Second Set:(df1_corr)\")\n",
    "# print(\", \".join(second_set_quoted))\n",
    "# print(\"\\nCommon Column Names: (To consider for removal/fill)\")\n",
    "# print(\", \".join(common_columns_quoted))\n",
    "\n",
    "# plt.figure(figsize=(20,18))\n",
    "# plt.barh(first_set, [1]*len(first_set), color='blue', label='First Set')\n",
    "# plt.barh(second_set, [1]*len(second_set), color='red', label='Second Set')\n",
    "# plt.barh(common_columns, [1]*len(common_columns), color='yellow', label='Common Columns')\n",
    "# plt.xlabel('Count')\n",
    "# plt.title('Column Names')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Find the Nan percentages in the common columns\n",
    "# nan_percentages_common = df1[common_columns].isna().mean() * 100\n",
    "\n",
    "# Sort and display the result\n",
    "# nan_percentages_common_sorted = nan_percentages_common.sort_values(ascending=False)\n",
    "# display(nan_percentages_common_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FRSTUSE1 replace -9 with 0? Confirm with value counts.\n",
    "# df1['FRSTUSE1'] = df1['FRSTUSE1'].replace(-9, 0)\n",
    "# print(df1['FRSTUSE1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FRSTUSE2 replace -9 with 0? Confirm with value counts.\n",
    "# df1['FRSTUSE2'] = df1['FRSTUSE2'].replace(-9, 0)\n",
    "# print(df1['FRSTUSE2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FRSTUSE3 replace -9 with 0? Confirm with value counts.\n",
    "# df1['FRSTUSE3'] = df1['FRSTUSE3'].replace(-9, 0)\n",
    "# print(df1['FRSTUSE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB2 replace -9 with 19? Confirm with value counts.\n",
    "# df1['SUB2'] = df1['SUB2'].replace(-9, 19)\n",
    "# print(df1['SUB2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB2_D replace -9 with 19? Confirm with value counts.\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 19)\n",
    "# print(df1['SUB2_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB3 replace -9 with 19? Confirm with value counts.\n",
    "# df1['SUB3'] = df1['SUB3'].replace(-9, 19)\n",
    "# print(df1['SUB3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB3_D replace -9 with 0? Confirm with value counts.\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "# print(df1['SUB3_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column ROUTE3 replace -9 with 0? Confirm with value counts.\n",
    "# df1['ROUTE3'] = df1['ROUTE3'].replace(-9, 0)\n",
    "# print(df1['ROUTE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column DSMCRIT replace -9 with 0? Confirm with value counts.\n",
    "# df1['DSMCRIT'] = df1['DSMCRIT'].replace(-9, 0)\n",
    "# print(df1['DSMCRIT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column PSYPROB replace -9 with 0? Confirm with value counts.\n",
    "# df1['PSYPROB'] = df1['PSYPROB'].replace(-9, 0)\n",
    "# print(df1['PSYPROB'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column PRIMPAY replace -9 with 0? Confirm with value counts.\n",
    "# df1['PRIMPAY'] = df1['PRIMPAY'].replace(-9, 0)\n",
    "# print(df1['PRIMPAY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FREQ_ATND_SELF_HELP replace -9 with 0? Confirm with value counts.\n",
    "# df1['FREQ_ATND_SELF_HELP'] = df1['FREQ_ATND_SELF_HELP'].replace(-9, 0)\n",
    "# print(df1['FREQ_ATND_SELF_HELP'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FREQ_ATND_SELF_HELP_D replace -9 with 0? Confirm with value counts.\n",
    "# df1['FREQ_ATND_SELF_HELP_D'] = df1['FREQ_ATND_SELF_HELP_D'].replace(-9, 0)\n",
    "# print(df1['FREQ_ATND_SELF_HELP_D'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns that need clarification:\n",
    "* ROUTE2: Route of administration (secondary)\n",
    "* IDU: Current IV drug use reported at admission\n",
    "* HLTHINS: Health insurance\n",
    "\n",
    "\n",
    "These are the 'Drug use reported at admission' columns. Not reported = 0, and reported = 1. The percentage for each column is fairly high. I believe that these columns are important for the model.\n",
    "\n",
    "* ALCFLG: Alcohol reported at admission\n",
    "* COKEFLG: Cocaine/crack reported at admission\n",
    "* MARFLG: Marijuana/hashish reported at admission\n",
    "* HERFLG: Heroin reported at admission\n",
    "* METHFLG: Non-rx methadone reported at admission\n",
    "* OPSYNFLG: Other opiates/synthetics reported at admission\n",
    "* PCPFLG: PCP reported at admission\n",
    "* HALLFLG: Hallucinogens reported at admission\n",
    "* MTHAMFLG: Methamphetamine/speed reported at admission\n",
    "* AMPHFLG: Other amphetamines reported at admission\n",
    "* STIMFLG: Other stimulants reported at admission\n",
    "* BENZFLG: Benzodiazepines reported at admission\n",
    "* TRNQFLG: Other tranquilizers reported at admission\n",
    "* BARBFLG: Barbiturates reported at admission\n",
    "* SEDHPFLG: Other sedatives/hypnotics reported at admission\n",
    "* INHFLG: Inhalants reported at admission\n",
    "* OTCFLG: Over-the-counter medication reported at admission\n",
    "* OTHERFLG: Other drug reported at admission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What are the correlations of all columns to the target variable 'REASON'?\n",
    "# df1_corr = df1.corr()\n",
    "# variable = df1_corr['REASON'].sort_values()\n",
    "# print('Correlation of REASON with other variables.')\n",
    "# display(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize only the positive correlations\n",
    "# positive_correlations = variable[variable > 0]\n",
    "# plt.figure(figsize=(20,18))\n",
    "# positive_correlations.plot(kind='bar', color='blue')\n",
    "# plt.title('Visualization of only the positive correlations to the target variable')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display LOS\n",
    "# los = df1['LOS']\n",
    "# display(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In LOS (Length of Stay) column, replace the values with the following:\n",
    "# # if days is between 1 and 30, return 1\n",
    "# # if days is between 31 and 45, return 2\n",
    "# # if days is between 46 and 60, return 3\n",
    "# # if days is between 61 and 90, return 4\n",
    "# # if days is between 91 and 120, return 5\n",
    "# # if days is between 121 and 180, return 6\n",
    "# # if days is between 181 and 365, return 7\n",
    "# # if days is greater than 365, return 8\n",
    "# # if days is any other value, return None\n",
    "\n",
    "\n",
    "\n",
    "# def los_to_category(days):\n",
    "#     if 1 <= days <= 30: \n",
    "#         return 1    \n",
    "#     elif 31 <= days <= 45: \n",
    "#         return 2    \n",
    "#     elif 46 <= days <= 60: \n",
    "#         return 3    \n",
    "#     elif 61 <= days <= 90: \n",
    "#         return 4    \n",
    "#     elif 91 <= days <= 120: \n",
    "#         return 5    \n",
    "#     elif 121 <= days <= 180: \n",
    "#         return 6    \n",
    "#     elif 181 <= days <= 365: \n",
    "#         return 7    \n",
    "#     elif days > 365: \n",
    "#         return 8  \n",
    "#     else:   \n",
    "#         return None  \n",
    "\n",
    "# # Test cases\n",
    "# print(los_to_category(15))  # Expected output: 1\n",
    "# print(los_to_category(35))  # Expected output: 2\n",
    "# print(los_to_category(50))  # Expected output: 3\n",
    "# print(los_to_category(70))  # Expected output: 4\n",
    "# print(los_to_category(100))  # Expected output: 5\n",
    "# print(los_to_category(150))  # Expected output: 6\n",
    "# print(los_to_category(200))  # Expected output: 7\n",
    "# print(los_to_category(400))  # Expected output: 8\n",
    "# print(los_to_category(-5))  # Expected output: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for missing values in the LOS column\n",
    "# missing_values_los = los.isna().sum()\n",
    "# print('Missing values in the LOS column:', missing_values_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for -9 in the LOS column\n",
    "# missing_values_los = los.isin([-9]).sum()\n",
    "# print('Missing values in the LOS column:', missing_values_los)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It got weird. Now we are dropping columns that are not important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REASON\n",
       "1    1095432\n",
       "0     627071\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.copy()\n",
    "# from conversation with \"house of Hope recovery\" class 4 which is \"transfer to a different facility\" would also be considered a success.\n",
    "for value in df1['REASON']:\n",
    "    if value == 1:\n",
    "        value = value\n",
    "    elif value == 4:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "df1['REASON'] = df1['REASON'].replace(4, 1)\n",
    "df1['REASON'] = df1['REASON'].replace(2, 0)\n",
    "df1['REASON'] = df1['REASON'].replace(3, 0)\n",
    "df1['REASON'] = df1['REASON'].replace(5, 0)\n",
    "df1['REASON'] = df1['REASON'].replace(6, 0)\n",
    "df1['REASON'] = df1['REASON'].replace(7, 0)\n",
    "df1['REASON'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB1\n",
      "0    1145410\n",
      "1     577093\n",
      "Name: count, dtype: int64\n",
      "SUB1_D\n",
      "0    1153301\n",
      "1     569202\n",
      "Name: count, dtype: int64\n",
      "SUB2\n",
      "1    959459\n",
      "0    763044\n",
      "Name: count, dtype: int64\n",
      "SUB2_D\n",
      "1    990170\n",
      "0    732333\n",
      "Name: count, dtype: int64\n",
      "SUB3\n",
      "1    1278583\n",
      "0     443920\n",
      "Name: count, dtype: int64\n",
      "SUB3_D\n",
      "1    1403980\n",
      "0     318523\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# time to clean up the data starting with SUB1_D \n",
    "# alcohol could be a success, mmj could be a success\n",
    "for value in df1['SUB1_D']:\n",
    "    if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(14, 1)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(16, 1)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(18, 1)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(12, 1)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(13, 1)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(2, 1)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(3, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(4, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(5, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(6, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(7, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(8, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(9, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(10, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(11, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(15, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(17, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(19, 0)\n",
    "\n",
    "df1['SUB1'] = df1['SUB1'].replace(14, 1)\n",
    "df1['SUB1'] = df1['SUB1'].replace(16, 1)\n",
    "df1['SUB1'] = df1['SUB1'].replace(18, 1)\n",
    "df1['SUB1'] = df1['SUB1'].replace(12, 1)\n",
    "df1['SUB1'] = df1['SUB1'].replace(13, 1)\n",
    "df1['SUB1'] = df1['SUB1'].replace(2, 1)\n",
    "df1['SUB1'] = df1['SUB1'].replace(3, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(4, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(5, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(6, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(7, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(8, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(9, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(10, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(11, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(15, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(17, 0)\n",
    "df1['SUB1'] = df1['SUB1'].replace(19, 0)\n",
    "\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(14, 1)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(16, 1)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(18, 1)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(12, 1)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(13, 1)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(2, 1)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(3, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(4, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(5, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(6, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(7, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(8, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(9, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(10, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(11, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(15, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(17, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(19, 0)\n",
    "\n",
    "df1['SUB2'] = df1['SUB2'].replace(14, 1)\n",
    "df1['SUB2'] = df1['SUB2'].replace(16, 1)\n",
    "df1['SUB2'] = df1['SUB2'].replace(18, 1)\n",
    "df1['SUB2'] = df1['SUB2'].replace(12, 1)\n",
    "df1['SUB2'] = df1['SUB2'].replace(13, 1)\n",
    "df1['SUB2'] = df1['SUB2'].replace(2, 1)\n",
    "df1['SUB2'] = df1['SUB2'].replace(3, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(4, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(5, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(6, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(7, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(8, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(9, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(10, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(11, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(15, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(17, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(19, 0)\n",
    "\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(14, 1)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(16, 1)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(18, 1)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(12, 1)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(13, 1)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(2, 1)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(3, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(4, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(5, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(6, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(7, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(8, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(9, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(10, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(11, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(15, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(17, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(19, 0)\n",
    "\n",
    "df1['SUB3'] = df1['SUB3'].replace(14, 1)\n",
    "df1['SUB3'] = df1['SUB3'].replace(16, 1)\n",
    "df1['SUB3'] = df1['SUB3'].replace(18, 1)\n",
    "df1['SUB3'] = df1['SUB3'].replace(12, 1)\n",
    "df1['SUB3'] = df1['SUB3'].replace(13, 1)\n",
    "df1['SUB3'] = df1['SUB3'].replace(2, 1)\n",
    "df1['SUB3'] = df1['SUB3'].replace(3, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(4, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(5, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(6, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(7, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(8, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(9, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(10, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(11, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(15, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(17, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(19, 0)\n",
    "# df1['SUB1_D'].notna().value_counts()\n",
    "# time to clean up the data starting with SUB1 \n",
    "# alcohol could be a success, mmj could be a success, using the same values for SUB1_D\n",
    "for value in df1['SUB1']:\n",
    "    if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "for value in df1['SUB2_D']:\n",
    "    if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "# df1['SUB1_D'].notna().value_counts()\n",
    "# time to clean up the data starting with SUB1 \n",
    "# alcohol could be a success, mmj could be a success, using the same values for SUB1_D\n",
    "for value in df1['SUB2']:\n",
    "    if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "for value in df1['SUB3_D']:\n",
    "    if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "# df1['SUB1_D'].notna().value_counts()\n",
    "# time to clean up the data starting with SUB1 \n",
    "# alcohol could be a success, mmj could be a success, using the same values for SUB1_D\n",
    "for value in df1['SUB3']:\n",
    "    if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "df1['SUB1'] = df1['SUB1'].replace(-9, 0)\n",
    "df1['SUB1_D'] = df1['SUB1_D'].replace(-9, 0)\n",
    "df1['SUB2'] = df1['SUB2'].replace(-9, 0)\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 0)\n",
    "df1['SUB3'] = df1['SUB3'].replace(-9, 0)\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "print(df1['SUB1'].value_counts())\n",
    "print(df1['SUB1_D'].value_counts())\n",
    "print(df1['SUB2'].value_counts())\n",
    "print(df1['SUB2_D'].value_counts())\n",
    "print(df1['SUB3'].value_counts())\n",
    "print(df1['SUB3_D'].value_counts())\n",
    "\n",
    "# df1['SUB1_D'].notna().value_counts()\n",
    "# # In column SUB2 replace -9 with 19\n",
    "df1['SUB2'] = df1['SUB2'].replace(-9, 19)\n",
    "# print(df1['SUB2'].value_counts())\n",
    "# # In column SUB2_D replace -9 with 19\n",
    "# # df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 19)\n",
    "# print(df1['SUB2_D'].value_counts())\n",
    "# # In column SUB3 replace -9 with 19\n",
    "# # df1['SUB3'] = df1['SUB3'].replace(-9, 19)\n",
    "# print(df1['SUB3'].value_counts())\n",
    "# # In column SUB3_D replace -9 with 0\n",
    "# # df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "# print(df1['SUB3_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with FREQ1_D, FREQ2_D, FREQ3_D\n",
    "# replacing -9 in the columns with 1 since 1 indicates 'no use' and if data was missing we assumed it was irrelivent to begin with\n",
    "df1['FREQ1_D'] = df1['FREQ1_D'].replace(-9, 1)\n",
    "df1['FREQ2_D'] = df1['FREQ2_D'].replace(-9, 1)\n",
    "df1['FREQ3_D'] = df1['FREQ3_D'].replace(-9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with FREQ1, FREQ2, FREQ3\n",
    "# replacing -9 in the columns with 1 since 1 indicates 'no use' and if data was missing we assumed it was irrelivent to begin with\n",
    "df1['FREQ1'] = df1['FREQ1'].replace(-9, 1)\n",
    "df1['FREQ2'] = df1['FREQ2'].replace(-9, 1)\n",
    "df1['FREQ3'] = df1['FREQ3'].replace(-9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with 'CASEID'\n",
    "df1['CASEID'].notna().value_counts()\n",
    "# CASEID has no null values and does need info filled\n",
    "# we can drop this field\n",
    "df1 = df1.drop(columns=\"CASEID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AGE\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'AGE'\n",
    "df1['AGE'].notna().value_counts()\n",
    "# AGE has no null values and does need info filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GENDER\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'GENDER'\n",
    "# gender is not a columns where we can fill in the data and not decrease the accuracy. so for all -9 values I am replacing with 0\n",
    "df1['GENDER'] = df1['GENDER'].replace(-9, 0)\n",
    "df1['GENDER'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with 'DISYR'\n",
    "df1['DISYR'].notna().value_counts()\n",
    "# DISYR has no null values and does need info filled\n",
    "# drop this column\n",
    "df1 = df1.drop(columns=\"DISYR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RACE\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'RACE'\n",
    "# for race, '7' indicateds 'other single race  so for all -9 values i replaces them with 7\n",
    "df1['RACE'] = df1['RACE'].replace(-9, 7)\n",
    "df1['RACE'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ETHNIC\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'ETHNIC'\n",
    "# for ETHNIC, '4' indicateds 'other single race  so for all -9 values I replaced them with 4\n",
    "df1['ETHNIC'] = df1['ETHNIC'].replace(-9, 4)\n",
    "df1['ETHNIC'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MARSTAT\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'MARSTAT'\n",
    "# for MARSTAT, hard to determine what to fill -9 with so filling with 0 for now\n",
    "df1['MARSTAT'] = df1['MARSTAT'].replace(-9, 0)\n",
    "df1['MARSTAT'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EDUC\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'EDUC'\n",
    "# for EDUC, hard to determine what to fill -9 with. options are 1-5 so filling with 2 as it seems like a fair average\n",
    "df1['EDUC'] = df1['EDUC'].replace(-9, 2)\n",
    "df1['EDUC'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMPLOY\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'EMPLOY' and 'EMPLOY_D'\n",
    "# for EMPLOY, hard to determine what to fill -9 with. options are 1-4. filling with 2 for now since if the data is missing is seems like that would indicate 'unemployed'\n",
    "df1['EMPLOY'] = df1['EMPLOY'].replace(-9, 0)\n",
    "df1['EMPLOY_D'] = df1['EMPLOY_D'].replace(-9, 0)\n",
    "df1['EMPLOY'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with 'DETNLF' and 'DETNLF_D'\n",
    "# for DETNLF, its just details on the previous columns not in labor force option so im replacing missing values with 0 as to not affect the results as much. we may want to drop this column\n",
    "# df1['DETNLF'] = df1['DETNLF'].replace(-9, 0)\n",
    "# df1['DETNLF_D'] = df1['DETNLF_D'].replace(-9, 0)\n",
    "# df1['DETNLF'].notna().value_counts()\n",
    "# drop this column\n",
    "df1 = df1.drop(columns=\"DETNLF\")\n",
    "df1 = df1.drop(columns=\"DETNLF_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PREG\n",
       "True    1722503\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'PREG'\n",
    "# for PREG, -9 most likely indicates male patients. replacing with 0 as to not affect the results as much. \n",
    "df1['PREG'] = df1['PREG'].replace(-9, 2)\n",
    "df1['PREG'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VET\n",
       "2    1679845\n",
       "1      42658\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'VET'\n",
    "# for VET, -9 most likely indicates not a veteren so im replacing -9 with 2 for 'no'\n",
    "df1['VET'] = df1['VET'].replace(-9, 2)\n",
    "df1['VET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIVARAG\n",
       "3    1013787\n",
       "1     436264\n",
       "2     272452\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'LIVARAG'\n",
    "# for LIVARAG, its hard to determine what -9 could indicate so im replacing -9 with 0 as to not affect the data as much with the -9's in this column\n",
    "df1['LIVARAG'] = df1['LIVARAG'].replace(-9, 1)\n",
    "df1['LIVARAG_D'] = df1['LIVARAG_D'].replace(-9, 1)\n",
    "df1['LIVARAG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRIMINC\n",
       "4    855478\n",
       "5    397327\n",
       "1    312986\n",
       "2     81605\n",
       "3     75107\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'PRIMINC'\n",
    "# for PRIMINC, its hard to determine what -9 could indicate so im replacing -9 with 0 as to not affect the data as much with the -9's in this column\n",
    "df1['PRIMINC'] = df1['PRIMINC'].replace(-9, 4)\n",
    "df1['PRIMINC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ARRESTS_D\n",
       "0    1643399\n",
       "1      61908\n",
       "2      17196\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'ARRESTS' and 'ARRESTS_D'\n",
    "# for 'ARRESTS' and 'ARRESTS_D', 0 indicates none so im setting -9 to none as that seems most likely to be the case if the information is missing\n",
    "df1['ARRESTS'] = df1['ARRESTS'].replace(-9, 0)\n",
    "df1['ARRESTS_D'] = df1['ARRESTS_D'].replace(-9, 0)\n",
    "df1['ARRESTS'].value_counts()\n",
    "df1['ARRESTS_D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with ' STFIPS'\n",
    "# for ' STFIPS', there are no null values in this column but we may want to remove it because the large values could through the accuracy off\n",
    "df1['STFIPS'].notna().value_counts()\n",
    "# drop this column\n",
    "df1 = df1.drop(columns=\"STFIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REGION\n",
       "1    532413\n",
       "3    505555\n",
       "4    378260\n",
       "2    303497\n",
       "0      2778\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'REGION'\n",
    "# for 'REGION', there are no null values in this column but we may want to remove it because the values could through the accuracy off and it doesnt seem to be relevant data\n",
    "df1['REGION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIVISION\n",
       "2    372934\n",
       "5    331617\n",
       "8    229376\n",
       "1    159479\n",
       "4    152327\n",
       "3    151170\n",
       "9    148884\n",
       "6    100660\n",
       "7     73278\n",
       "0      2778\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'DIVISION'\n",
    "# for 'DIVISION', there are no null values in this column but we may want to remove it because the values could through the accuracy off and it doesnt seem to be relevant data\n",
    "df1['DIVISION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SERVICES_D\n",
       "7    884266\n",
       "2    240303\n",
       "6    235771\n",
       "4    179737\n",
       "5    121470\n",
       "1     43446\n",
       "8     13267\n",
       "3      4243\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'SERVICES and SERVICES_D'\n",
    "# for 'SERVICES and SERVICES_D', there are no null values in this column but we may want to remove it because the values could through the accuracy off and it doesnt seem to be relevant data for the questions we are trying to answer\n",
    "df1['SERVICES'].value_counts()\n",
    "df1['SERVICES_D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DAYWAIT\n",
       "0    1469735\n",
       "1     175018\n",
       "2      34722\n",
       "3      26618\n",
       "4      16410\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'DAYWAIT'\n",
    "# for 'DAYWAIT', its seems safe to replace a value of missing data with a value of '0' to indicate that they didnt wait\n",
    "df1['DAYWAIT'] = df1['DAYWAIT'].replace(-9, 0)\n",
    "df1['DAYWAIT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "METHUSE\n",
       "2    1327459\n",
       "1     216239\n",
       "0     178805\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time to clean up the data with 'METHUSE'\n",
    "# for 'METHUSE', its seems safe to replace a value of missing data with a value of 'none'\n",
    "df1['METHUSE'] = df1['METHUSE'].replace(-9, 0)\n",
    "df1['METHUSE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with 'LOS'\n",
    "# for 'LOS', stands for length of stay this data seems irrelivent to the questions we are trying to answer so I suggest dropping the column to perserve prediction accuracy\n",
    "# 1-30, 31-45, 46-60, 61-90, 91-120, 121-180, 181-365, more than\n",
    "def los_to_category(days):\n",
    "    if days <= 30:\n",
    "        return 1\n",
    "    elif days <= 31:\n",
    "        return 2\n",
    "    elif days <= 32:\n",
    "        return 3\n",
    "    elif days <= 33:\n",
    "        return 4\n",
    "    elif days <= 34:\n",
    "        return 5\n",
    "    elif days <= 35:\n",
    "        return 6\n",
    "    elif days <= 36:\n",
    "        return 7\n",
    "    else:\n",
    "        return 8\n",
    "# Test cases\n",
    "\n",
    "# df1['LOS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(los_to_category(15))  # Expected output: 1\n",
    "print(los_to_category(35))  # Expected output: 2\n",
    "print(los_to_category(50))  # Expected output: 3\n",
    "print(los_to_category(70))  # Expected output: 4\n",
    "print(los_to_category(100))  # Expected output: 5\n",
    "print(los_to_category(150))  # Expected output: 6\n",
    "print(los_to_category(200))  # Expected output: 7\n",
    "print(los_to_category(400))  # Expected output: 8\n",
    "print(los_to_category(-5))  # Expected output: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['LOS'] = df1['LOS'].apply(los_to_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOS\n",
       "1    934659\n",
       "4    140555\n",
       "7    128974\n",
       "5    114387\n",
       "6    114266\n",
       "2    111830\n",
       "8     93500\n",
       "3     84332\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['LOS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with PSOURCE: Referral source\n",
    "# for 'PSOURCE' this doesnt seem to me to be relevant data so im replacing -9 with 1 for now to indicate that it was self motivated\n",
    "df1['PSOURCE'] = df1['PSOURCE'].replace(-9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with DETCRIM: Detailed criminal justice referral\n",
    "# for 'DETCRIM' im replacing -9 with 0 for now. most of this data is missing. we may want to drop this column\n",
    "df1['DETCRIM'] = df1['DETCRIM'].replace(-9, 0)\n",
    "# drop this column\n",
    "df1 = df1.drop(columns=\"DETCRIM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with NOPRIOR: Previous substance use treatment episodes\n",
    "# for 'NOPRIOR' im replacing -9 with 0 for now. its could go either way. we will ask clients\n",
    "df1['NOPRIOR'] = df1['NOPRIOR'].replace(-9, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to clean up the data with ROUTE1: Route of administration (primary)\n",
    "# time to clean up the data with ROUTE2: Route of administration (secondary)\n",
    "# time to clean up the data with ROUTE3: Route of administration (tertiary)\n",
    "# for 'ROUTE1' im replacing -9 with 5 for now to indicate 'other'. its could go either way. we will ask clients\n",
    "df1['ROUTE1'] = df1['ROUTE1'].replace(-9, 5)\n",
    "df1['ROUTE2'] = df1['ROUTE2'].replace(-9, 5)\n",
    "df1['ROUTE3'] = df1['ROUTE3'].replace(-9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRSTUSE2\n",
      "3    1067078\n",
      "2     170892\n",
      "4     153705\n",
      "7     103384\n",
      "5      95617\n",
      "6      78339\n",
      "1      53488\n",
      "Name: count, dtype: int64\n",
      "FRSTUSE3\n",
      "3    1464306\n",
      "2      78630\n",
      "4      57055\n",
      "7      37209\n",
      "5      32205\n",
      "6      27655\n",
      "1      25443\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# time to clean up the data with FRSTUSE1: Age at first use (primary)\n",
    "# FRSTUSE1 is a column where its hard to determine what a value of -9 should be replaced with so im replacing it with 0 for now to have less of an impact on the data\n",
    "df1['FRSTUSE1'] = df1['FRSTUSE1'].replace(-9, 3)\n",
    "# adding Beau's code\n",
    "# In column FRSTUSE2 replace -9 with 0\n",
    "df1['FRSTUSE2'] = df1['FRSTUSE2'].replace(-9, 3)\n",
    "print(df1['FRSTUSE2'].value_counts())\n",
    "# In column FRSTUSE3 replace -9 with 0\n",
    "df1['FRSTUSE3'] = df1['FRSTUSE3'].replace(-9, 3)\n",
    "print(df1['FRSTUSE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB2\n",
      "1    959459\n",
      "0    763044\n",
      "Name: count, dtype: int64\n",
      "SUB2_D\n",
      "1    990170\n",
      "0    732333\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column SUB2 replace -9 with 19\n",
    "df1['SUB2'] = df1['SUB2'].replace(-9, 19)\n",
    "print(df1['SUB2'].value_counts())\n",
    "# In column SUB2_D replace -9 with 19\n",
    "df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 19)\n",
    "print(df1['SUB2_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUB3\n",
      "1    1278583\n",
      "0     443920\n",
      "Name: count, dtype: int64\n",
      "SUB3_D\n",
      "1    1403980\n",
      "0     318523\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column SUB3 replace -9 with 19\n",
    "df1['SUB3'] = df1['SUB3'].replace(-9, 19)\n",
    "print(df1['SUB3'].value_counts())\n",
    "# In column SUB3_D replace -9 with 0\n",
    "df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "print(df1['SUB3_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUTE3\n",
      "5    1365958\n",
      "2     154215\n",
      "1     122303\n",
      "3      48044\n",
      "4      31983\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column ROUTE3 replace -9 with 0\n",
    "df1['ROUTE3'] = df1['ROUTE3'].replace(-9, 0)\n",
    "print(df1['ROUTE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DSMCRIT\n",
      "5     790823\n",
      "4     289535\n",
      "8     143441\n",
      "19    125445\n",
      "7      82105\n",
      "6      56213\n",
      "9      52972\n",
      "2      40101\n",
      "10     39565\n",
      "3      29130\n",
      "11     23297\n",
      "12     17972\n",
      "1      12628\n",
      "13      7786\n",
      "15      5239\n",
      "17      2246\n",
      "14      2181\n",
      "16      1490\n",
      "18       334\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column DSMCRIT replace -9 with 0\n",
    "df1['DSMCRIT'] = df1['DSMCRIT'].replace(-9, 5)\n",
    "print(df1['DSMCRIT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSYPROB\n",
      "1    899280\n",
      "2    823223\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column PSYPROB replace -9 with 1\n",
    "df1['PSYPROB'] = df1['PSYPROB'].replace(-9, 1)\n",
    "print(df1['PSYPROB'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIMPAY\n",
      "1    1038444\n",
      "4     389616\n",
      "5     162022\n",
      "7      49127\n",
      "2      44071\n",
      "6      20758\n",
      "3      18465\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column PRIMPAY replace -9 with 1\n",
    "df1['PRIMPAY'] = df1['PRIMPAY'].replace(-9, 1)\n",
    "print(df1['PRIMPAY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREQ_ATND_SELF_HELP\n",
      "1    1037604\n",
      "3     389036\n",
      "4     126267\n",
      "2     104522\n",
      "5      65074\n",
      "Name: count, dtype: int64\n",
      "FREQ_ATND_SELF_HELP_D\n",
      "1    859331\n",
      "3    452977\n",
      "4    186509\n",
      "2    115895\n",
      "5    107791\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# In column FREQ_ATND_SELF_HELP replace -9 with 0\n",
    "df1['FREQ_ATND_SELF_HELP'] = df1['FREQ_ATND_SELF_HELP'].replace(-9, 3)\n",
    "print(df1['FREQ_ATND_SELF_HELP'].value_counts())\n",
    "# In column FREQ_ATND_SELF_HELP_D replace -9 with 0\n",
    "df1['FREQ_ATND_SELF_HELP_D'] = df1['FREQ_ATND_SELF_HELP_D'].replace(-9, 3)\n",
    "print(df1['FREQ_ATND_SELF_HELP_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALCFLG       0.025359\n",
       "LIVARAG_D    0.026111\n",
       "FREQ1_D      0.029926\n",
       "HLTHINS      0.032940\n",
       "ROUTE3       0.036662\n",
       "EMPLOY_D     0.040257\n",
       "ETHNIC       0.057235\n",
       "SUB1_D       0.066446\n",
       "PRIMPAY      0.067499\n",
       "AGE          0.068791\n",
       "ROUTE2       0.071289\n",
       "SUB1         0.073811\n",
       "SUB2         0.080155\n",
       "SUB3_D       0.088915\n",
       "METHUSE      0.096050\n",
       "SUB3         0.102280\n",
       "DIVISION     0.128182\n",
       "SUB2_D       0.136620\n",
       "REGION       0.153238\n",
       "REASON       1.000000\n",
       "Name: REASON, dtype: float64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reason = df1['REASON']\n",
    "df1_corr = df1.corr()\n",
    "df1_corr.unstack().sort_values()\n",
    "variable = df1_corr['REASON'].sort_values()\n",
    "variable.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CBSA2010</th>\n",
       "      <th>EDUC</th>\n",
       "      <th>MARSTAT</th>\n",
       "      <th>SERVICES</th>\n",
       "      <th>LOS</th>\n",
       "      <th>PSOURCE</th>\n",
       "      <th>NOPRIOR</th>\n",
       "      <th>ARRESTS</th>\n",
       "      <th>EMPLOY</th>\n",
       "      <th>METHUSE</th>\n",
       "      <th>...</th>\n",
       "      <th>TRNQFLG</th>\n",
       "      <th>BARBFLG</th>\n",
       "      <th>SEDHPFLG</th>\n",
       "      <th>INHFLG</th>\n",
       "      <th>OTCFLG</th>\n",
       "      <th>OTHERFLG</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>REGION</th>\n",
       "      <th>IDU</th>\n",
       "      <th>ALCDRUG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722498</th>\n",
       "      <td>-9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722499</th>\n",
       "      <td>-9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722500</th>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722501</th>\n",
       "      <td>-9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722502</th>\n",
       "      <td>-9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1722503 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CBSA2010  EDUC  MARSTAT  SERVICES  LOS  PSOURCE  NOPRIOR  ARRESTS  \\\n",
       "0              -9     4        1         7    8        1        0        0   \n",
       "1              -9     3        1         7    6        1        0        0   \n",
       "2              -9     2        1         7    6        1        0        0   \n",
       "3              -9     3        1         7    8        1        1        0   \n",
       "4              -9     3        3         7    8        1        1        0   \n",
       "...           ...   ...      ...       ...  ...      ...      ...      ...   \n",
       "1722498        -9     4        2         7    4        1        0        0   \n",
       "1722499        -9     3        3         7    1        7        0        0   \n",
       "1722500        -9     2        1         6    4        1        0        0   \n",
       "1722501        -9     1        1         7    1        7        0        0   \n",
       "1722502        -9     2        4         7    1        1        0        0   \n",
       "\n",
       "         EMPLOY  METHUSE  ...  TRNQFLG  BARBFLG  SEDHPFLG  INHFLG  OTCFLG  \\\n",
       "0             2        2  ...        0        0         0       0       0   \n",
       "1             1        2  ...        0        0         0       0       0   \n",
       "2             4        2  ...        0        0         0       0       0   \n",
       "3             3        2  ...        0        0         0       0       0   \n",
       "4             4        2  ...        0        0         0       0       0   \n",
       "...         ...      ...  ...      ...      ...       ...     ...     ...   \n",
       "1722498       1        2  ...        0        0         0       0       0   \n",
       "1722499       4        2  ...        0        0         0       0       0   \n",
       "1722500       3        2  ...        0        0         0       0       0   \n",
       "1722501       4        2  ...        0        0         0       0       0   \n",
       "1722502       1        2  ...        0        0         0       0       0   \n",
       "\n",
       "         OTHERFLG  DIVISION  REGION  IDU  ALCDRUG  \n",
       "0               0         9       4    0        1  \n",
       "1               0         9       4    0        3  \n",
       "2               0         9       4    0        3  \n",
       "3               0         9       4    0        3  \n",
       "4               0         9       4    0        1  \n",
       "...           ...       ...     ...  ...      ...  \n",
       "1722498         0         8       4    0        1  \n",
       "1722499         0         8       4    0        2  \n",
       "1722500         0         8       4    0        2  \n",
       "1722501         0         8       4    0        3  \n",
       "1722502         0         8       4    0        1  \n",
       "\n",
       "[1722503 rows x 70 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display df1\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation of REASON with other variables.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ROUTE1       0.024558\n",
       "ALCFLG       0.025359\n",
       "LIVARAG_D    0.026111\n",
       "FREQ1_D      0.029926\n",
       "HLTHINS      0.032940\n",
       "ROUTE3       0.036662\n",
       "EMPLOY_D     0.040257\n",
       "ETHNIC       0.057235\n",
       "SUB1_D       0.066446\n",
       "PRIMPAY      0.067499\n",
       "AGE          0.068791\n",
       "ROUTE2       0.071289\n",
       "SUB1         0.073811\n",
       "SUB2         0.080155\n",
       "SUB3_D       0.088915\n",
       "METHUSE      0.096050\n",
       "SUB3         0.102280\n",
       "DIVISION     0.128182\n",
       "SUB2_D       0.136620\n",
       "REGION       0.153238\n",
       "REASON       1.000000\n",
       "Name: REASON, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# What columns are highly correlated with the target variable 'REASON'?\n",
    "high_corr_df = df1_corr['REASON'].sort_values()\n",
    "print('Correlation of REASON with other variables.')\n",
    "display(high_corr_df.tail(21))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor Outputs:\n",
    "\n",
    "* First Run:\n",
    "    * Training Accuracy: 0.9980399157620962\n",
    "    * Testing Accuracy: 0.837835013541325\n",
    "\n",
    "* Second Run:\n",
    "    * Training Accuracy: \n",
    "    * Testing Accuracy: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Training Accuracy: 0.7882601041217647\n",
      "RandomForest Testing Accuracy: 0.787161720865832\n",
      "ExtraTrees Training Accuracy: 0.7665141269751423\n",
      "ExtraTrees Testing Accuracy: 0.7649905225238823\n"
     ]
    }
   ],
   "source": [
    "# Drop the target variable 'REASON' from the dataset\n",
    "X = df1.drop(columns='REASON')\n",
    "y = df1['REASON']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the testing data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=4, random_state=42)\n",
    "et_model = ExtraTreesClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=4, random_state=42)\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the training set\n",
    "# y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "# # Calculate the training accuracy\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# # Make predictions on the testing set\n",
    "# y_test_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate the testing accuracy\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# # Print the accuracies\n",
    "# print('Training Accuracy:', train_accuracy)\n",
    "# print('Testing Accuracy:', test_accuracy)\n",
    "\n",
    "# Train the RandomForest model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the RandomForest model\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate the RandomForest accuracies\n",
    "train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "# Print the RandomForest accuracies\n",
    "print('RandomForest Training Accuracy:', train_accuracy_rf)\n",
    "print('RandomForest Testing Accuracy:', test_accuracy_rf)\n",
    "\n",
    "# Train the ExtraTrees model\n",
    "et_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the ExtraTrees model\n",
    "y_train_pred_et = et_model.predict(X_train)\n",
    "y_test_pred_et = et_model.predict(X_test)\n",
    "\n",
    "# Calculate the ExtraTrees accuracies\n",
    "train_accuracy_et = accuracy_score(y_train, y_train_pred_et)\n",
    "test_accuracy_et = accuracy_score(y_test, y_test_pred_et)\n",
    "\n",
    "# Print the ExtraTrees accuracies\n",
    "print('ExtraTrees Training Accuracy:', train_accuracy_et)\n",
    "print('ExtraTrees Testing Accuracy:', test_accuracy_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Training Accuracy: 0.7882601041217647\n",
      "RandomForest Testing Accuracy: 0.787161720865832\n",
      "RandomForest is overfitting.\n",
      "ExtraTrees Training Accuracy: 0.7665141269751423\n",
      "ExtraTrees Testing Accuracy: 0.7649905225238823\n",
      "ExtraTrees is overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Print the RandomForest accuracies\n",
    "print('RandomForest Training Accuracy:', train_accuracy_rf)\n",
    "print('RandomForest Testing Accuracy:', test_accuracy_rf)\n",
    "\n",
    "# Check for overfitting and underfitting in RandomForest\n",
    "if train_accuracy_rf > test_accuracy_rf:\n",
    "    print('RandomForest is overfitting.')\n",
    "elif train_accuracy_rf < test_accuracy_rf:\n",
    "    print('RandomForest is underfitting.')\n",
    "else:\n",
    "    print('RandomForest is fitting well.')\n",
    "\n",
    "# Print the ExtraTrees accuracies\n",
    "print('ExtraTrees Training Accuracy:', train_accuracy_et)\n",
    "print('ExtraTrees Testing Accuracy:', test_accuracy_et)\n",
    "\n",
    "# Check for overfitting and underfitting in ExtraTrees\n",
    "if train_accuracy_et > test_accuracy_et:\n",
    "    print('ExtraTrees is overfitting.')\n",
    "elif train_accuracy_et < test_accuracy_et:\n",
    "    print('ExtraTrees is underfitting.')\n",
    "else:\n",
    "    print('ExtraTrees is fitting well.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Cross-Validation Accuracy: 0.7866280319297335\n",
      "ExtraTrees Cross-Validation Accuracy: 0.7629836538765005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation on the RandomForest model\n",
    "rf_scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "print('RandomForest Cross-Validation Accuracy:', np.mean(rf_scores))\n",
    "\n",
    "# Perform cross-validation on the ExtraTrees model\n",
    "et_scores = cross_val_score(et_model, X_train, y_train, cv=5)\n",
    "print('ExtraTrees Cross-Validation Accuracy:', np.mean(et_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=18.9min\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=25.7min\n",
      "[CV] END bootstrap=True, max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=29.4min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf_model, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Fit the GridSearchCV\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Get the best parameters\u001b[39;00m\n\u001b[1;32m     19\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/model_selection/_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    913\u001b[0m         )\n\u001b[1;32m    914\u001b[0m     )\n\u001b[0;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    939\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/dev/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Define the parameter grid\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 200, 300, 400, 500],\n",
    "#     'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4],\n",
    "#     'bootstrap': [True, False]\n",
    "# }\n",
    "\n",
    "# # Initialize the GridSearchCV\n",
    "# grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# # Fit the GridSearchCV\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# # Get the best parameters\n",
    "# best_params = grid_search.best_params_\n",
    "\n",
    "# # Train a RandomForestClassifier with the best parameters\n",
    "# rf_best = RandomForestClassifier(**best_params, random_state=42)\n",
    "# rf_best.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the training set\n",
    "# y_train_pred = rf_best.predict(X_train)\n",
    "\n",
    "# # Calculate the training accuracy\n",
    "# train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# # Make predictions on the testing set\n",
    "# y_test_pred = rf_best.predict(X_test)\n",
    "\n",
    "# # Calculate the testing accuracy\n",
    "# test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# # Print the accuracies\n",
    "# print('Training Accuracy:', train_accuracy)\n",
    "# print('Testing Accuracy:', test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
