{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     import joblib\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     %matplotlib inline\n",
    "#     import numpy as np\n",
    "#     import pandas as pd\n",
    "#     import seaborn as sns\n",
    "#     from xgboost import XGBClassifier\n",
    "#     from scipy import stats\n",
    "#     from sklearn.cluster import KMeans\n",
    "#     from sklearn.decomposition import PCA\n",
    "#     from sklearn.datasets import make_regression, make_swiss_roll\n",
    "#     from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "#     from sklearn.model_selection import GridSearchCV\n",
    "#     from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "#     from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "#     from sklearn.model_selection import train_test_split\n",
    "#     from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "#     from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "#     from sklearn.svm import SVC, SVR\n",
    "#     from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#     print('All libraries imported successfully!')\n",
    "# except ImportError:\n",
    "#     print('Some libraries failed to import.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data from a CSV file\n",
    "# df = pd.read_csv('tedsd_puf_2019.csv')\n",
    "# print('tedsd_puf_2019.csv loaded')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Summarize the data\n",
    "# display(df.shape)\n",
    "# display(df.describe())\n",
    "# display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a copy of the data\n",
    "# df1 = df.copy()\n",
    "# print('Copy of the DataFrame')\n",
    "# display(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In this dataset, -9 is used to represent missing values. Which columns have -9 as a value and how many?\n",
    "# missing_values = df1.isin([-9]).sum()\n",
    "# display(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What specific columns have -9 as a value?\n",
    "# missing_columns = missing_values[missing_values > 0]\n",
    "# display(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # How many columns have -9 as a value?\n",
    "# display(len(missing_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize all columns with -9 as a value\n",
    "# missing_columns.plot(kind='bar', figsize=(15, 7))\n",
    "# plt.title('Columns with -9 as a value')\n",
    "# plt.xlabel('Columns')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -9 with NaN\n",
    "# df1 = df1.replace(-9, np.nan)\n",
    "# print('Replacing -9 with NaN. A -9 represents a missing value code in the dataset.')\n",
    "# display(df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that contain NaN values\n",
    "# columns_with_nan = df1.isna().any()\n",
    "\n",
    "# # Count the number of columns that contain NaN values\n",
    "# num_columns_with_nan = columns_with_nan.sum()\n",
    "\n",
    "# # Print the number of columns that contain NaN values\n",
    "# print('There are',num_columns_with_nan,'columns that contain NaN.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns that contain NaN values\n",
    "# columns_with_nan = df1.columns[df1.isna().any()].tolist()\n",
    "\n",
    "# # Calculate the percentage of NaN values in each of these columns\n",
    "# nan_percentage = df1[columns_with_nan].isna().mean() * 100\n",
    "\n",
    "# # Sort the percentages in ascending order\n",
    "# nan_percentage_sorted = nan_percentage.sort_values(ascending=False)\n",
    "\n",
    "# # Print the percentage of NaN values\n",
    "# print('Percentage of NaN values in each column.')\n",
    "# display(nan_percentage_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot\n",
    "# plt.figure(figsize=(20,18))\n",
    "# sns.barplot(x=nan_percentage_sorted.values, y=nan_percentage_sorted.index)\n",
    "# plt.title('Percentage of NaN values in columns')\n",
    "# # Show the plot\n",
    "# print('Bar plot showing the percentage of NaN values in columns.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reason = df1['REASON']\n",
    "# df1_corr = df1.corr()\n",
    "# df1_corr.unstack().sort_values()\n",
    "# variable = df1_corr['REASON'].sort_values()\n",
    "# print('Correlation of REASON with other variables.')\n",
    "# variable.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of colors based on the values in 'variable'\n",
    "# colors = ['red' if x < 0 else 'blue' for x in variable[:-1]]\n",
    "\n",
    "# # Visualize correlation to the target variable 'REASON'\n",
    "# plt.figure(figsize=(20,18))\n",
    "# variable[:-1].plot(kind='bar', color=colors)\n",
    "# plt.title('Correlation to the target variable')\n",
    "# print('Visualizing the positive and negative correlations of REASON with other variables.')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to include only positive correlations\n",
    "# positive_correlations = variable[variable > 0]\n",
    "\n",
    "# # Visualize positive correlations\n",
    "# plt.figure(figsize=(20,18))\n",
    "# positive_correlations[:-1].plot(kind='bar', color='blue')\n",
    "# plt.title('This is a visualization of only the positive correlations to the target variable')\n",
    "# print('Positive correlations of REASON with other variables.')\n",
    "# print(\"\\nSERVICES is the highest positive correlation and EMPLOY is the lowest positive correlation at the moment.\")\n",
    "# print(\"\\nMore exploration is needed to understand the relationship between the target variable and the features.\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('These are the columns with NaN values, with the highest percentage of NaN values at the top.')\n",
    "# print(\"\\nFor example, the column FREQ3_D has roughly 83.66 precent of its values as NaN.\")\n",
    "# print('\\nThis means that 83.66 percent of the values in the column FREQ3_D are missing or not reported.')\n",
    "# display(nan_percentage_sorted)\n",
    "# display(df1_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the column names from the first set\n",
    "# first_set = [\n",
    "#     \"FREQ3_D\", \"DETCRIM\", \"FREQ3\", \"FRSTUSE3\", \"ROUTE3\", \"DETNLF_D\", \"DETNLF\",\n",
    "#     \"FREQ2_D\", \"PREG\", \"PRIMPAY\", \"DAYWAIT\", \"HLTHINS\", \"FRSTUSE2\", \"FREQ2\",\n",
    "#     \"ROUTE2\", \"CBSA2010\", \"PRIMINC\", \"FREQ1_D\", \"DSMCRIT\", \"FREQ_ATND_SELF_HELP_D\",\n",
    "#     \"MARSTAT\", \"LIVARAG_D\", \"ARRESTS_D\", \"FREQ_ATND_SELF_HELP\", \"EMPLOY_D\", \"PSYPROB\",\n",
    "#     \"SUB1_D\", \"FREQ1\", \"SUB3\", \"ARRESTS\", \"METHUSE\", \"EDUC\", \"LIVARAG\", \"VET\", \"EMPLOY\",\n",
    "#     \"FRSTUSE1\", \"ROUTE1\", \"NOPRIOR\", \"IDU\", \"SUB2_D\", \"PSOURCE\", \"SUB1\", \"SUB3_D\",\n",
    "#     \"SUB2\", \"ETHNIC\", \"RACE\", \"GENDER\"\n",
    "# ]\n",
    "\n",
    "# Define the column names from the second set\n",
    "# second_set = [\n",
    "#     \"DISYR\", \"CASEID\", \"STFIPS\", \"CBSA2010\", \"EDUC\", \"MARSTAT\", \"SERVICES\",\n",
    "#     \"DETCRIM\", \"LOS\", \"PSOURCE\", \"TRNQFLG\", \"BARBFLG\", \"SEDHPFLG\", \"INHFLG\",\n",
    "#     \"OTCFLG\", \"OTHERFLG\", \"DIVISION\", \"REGION\", \"IDU\", \"ALCDRUG\"\n",
    "# ]\n",
    "\n",
    "# Find the common column names\n",
    "# common_columns = list(set(first_set).intersection(second_set))\n",
    "\n",
    "# # Add quotations around each column name\n",
    "# first_set_quoted = [f'\"{col}\"' for col in first_set]\n",
    "# second_set_quoted = [f'\"{col}\"' for col in second_set]\n",
    "# common_columns_quoted = [f'\"{col}\"' for col in common_columns]\n",
    "\n",
    "# # Print the results\n",
    "# print(\"Looking for common columns in two sets of column names to see if they need be removed or filled.\")\n",
    "# print(\"\\nColumn Names from the First Set:(nan_percentage_sorted)\")\n",
    "# print(\", \".join(first_set_quoted))\n",
    "# print(\"\\nColumn Names from the Second Set:(df1_corr)\")\n",
    "# print(\", \".join(second_set_quoted))\n",
    "# print(\"\\nCommon Column Names: (To consider for removal/fill)\")\n",
    "# print(\", \".join(common_columns_quoted))\n",
    "\n",
    "# plt.figure(figsize=(20,18))\n",
    "# plt.barh(first_set, [1]*len(first_set), color='blue', label='First Set')\n",
    "# plt.barh(second_set, [1]*len(second_set), color='red', label='Second Set')\n",
    "# plt.barh(common_columns, [1]*len(common_columns), color='yellow', label='Common Columns')\n",
    "# plt.xlabel('Count')\n",
    "# plt.title('Column Names')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# Find the Nan percentages in the common columns\n",
    "# nan_percentages_common = df1[common_columns].isna().mean() * 100\n",
    "\n",
    "# Sort and display the result\n",
    "# nan_percentages_common_sorted = nan_percentages_common.sort_values(ascending=False)\n",
    "# display(nan_percentages_common_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FRSTUSE1 replace -9 with 0? Confirm with value counts.\n",
    "# df1['FRSTUSE1'] = df1['FRSTUSE1'].replace(-9, 0)\n",
    "# print(df1['FRSTUSE1'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FRSTUSE3 replace -9 with 0? Confirm with value counts.\n",
    "# df1['FRSTUSE3'] = df1['FRSTUSE3'].replace(-9, 0)\n",
    "# print(df1['FRSTUSE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB2 replace -9 with 19? Confirm with value counts.\n",
    "# df1['SUB2'] = df1['SUB2'].replace(-9, 19)\n",
    "# print(df1['SUB2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB2_D replace -9 with 19? Confirm with value counts.\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 19)\n",
    "# print(df1['SUB2_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB3 replace -9 with 19? Confirm with value counts.\n",
    "# df1['SUB3'] = df1['SUB3'].replace(-9, 19)\n",
    "# print(df1['SUB3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB3_D replace -9 with 0? Confirm with value counts.\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "# print(df1['SUB3_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column ROUTE3 replace -9 with 0? Confirm with value counts.\n",
    "# df1['ROUTE3'] = df1['ROUTE3'].replace(-9, 0)\n",
    "# print(df1['ROUTE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column DSMCRIT replace -9 with 0? Confirm with value counts.\n",
    "# df1['DSMCRIT'] = df1['DSMCRIT'].replace(-9, 0)\n",
    "# print(df1['DSMCRIT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column PSYPROB replace -9 with 0? Confirm with value counts.\n",
    "# df1['PSYPROB'] = df1['PSYPROB'].replace(-9, 0)\n",
    "# print(df1['PSYPROB'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column PRIMPAY replace -9 with 0? Confirm with value counts.\n",
    "# df1['PRIMPAY'] = df1['PRIMPAY'].replace(-9, 0)\n",
    "# print(df1['PRIMPAY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FREQ_ATND_SELF_HELP replace -9 with 0? Confirm with value counts.\n",
    "# df1['FREQ_ATND_SELF_HELP'] = df1['FREQ_ATND_SELF_HELP'].replace(-9, 0)\n",
    "# print(df1['FREQ_ATND_SELF_HELP'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns that need clarification:\n",
    "* ROUTE2: Route of administration (secondary)\n",
    "* IDU: Current IV drug use reported at admission\n",
    "* HLTHINS: Health insurance\n",
    "\n",
    "\n",
    "These are the 'Drug use reported at admission' columns. Not reported = 0, and reported = 1. The percentage for each column is fairly high. I believe that these columns are important for the model.\n",
    "\n",
    "* ALCFLG: Alcohol reported at admission\n",
    "* COKEFLG: Cocaine/crack reported at admission\n",
    "* MARFLG: Marijuana/hashish reported at admission\n",
    "* HERFLG: Heroin reported at admission\n",
    "* METHFLG: Non-rx methadone reported at admission\n",
    "* OPSYNFLG: Other opiates/synthetics reported at admission\n",
    "* PCPFLG: PCP reported at admission\n",
    "* HALLFLG: Hallucinogens reported at admission\n",
    "* MTHAMFLG: Methamphetamine/speed reported at admission\n",
    "* AMPHFLG: Other amphetamines reported at admission\n",
    "* STIMFLG: Other stimulants reported at admission\n",
    "* BENZFLG: Benzodiazepines reported at admission\n",
    "* TRNQFLG: Other tranquilizers reported at admission\n",
    "* BARBFLG: Barbiturates reported at admission\n",
    "* SEDHPFLG: Other sedatives/hypnotics reported at admission\n",
    "* INHFLG: Inhalants reported at admission\n",
    "* OTCFLG: Over-the-counter medication reported at admission\n",
    "* OTHERFLG: Other drug reported at admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What are the correlations of all columns to the target variable 'REASON'?\n",
    "# df1_corr = df1.corr()\n",
    "# variable = df1_corr['REASON'].sort_values()\n",
    "# print('Correlation of REASON with other variables.')\n",
    "# display(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize only the positive correlations\n",
    "# positive_correlations = variable[variable > 0]\n",
    "# plt.figure(figsize=(20,18))\n",
    "# positive_correlations.plot(kind='bar', color='blue')\n",
    "# plt.title('Visualization of only the positive correlations to the target variable')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display LOS\n",
    "# los = df1['LOS']\n",
    "# display(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In LOS (Length of Stay) column, replace the values with the following:\n",
    "# # if days is between 1 and 30, return 1\n",
    "# # if days is between 31 and 45, return 2\n",
    "# # if days is between 46 and 60, return 3\n",
    "# # if days is between 61 and 90, return 4\n",
    "# # if days is between 91 and 120, return 5\n",
    "# # if days is between 121 and 180, return 6\n",
    "# # if days is between 181 and 365, return 7\n",
    "# # if days is greater than 365, return 8\n",
    "# # if days is any other value, return None\n",
    "\n",
    "\n",
    "\n",
    "# def los_to_category(days):\n",
    "#     if 1 <= days <= 30: \n",
    "#         return 1    \n",
    "#     elif 31 <= days <= 45: \n",
    "#         return 2    \n",
    "#     elif 46 <= days <= 60: \n",
    "#         return 3    \n",
    "#     elif 61 <= days <= 90: \n",
    "#         return 4    \n",
    "#     elif 91 <= days <= 120: \n",
    "#         return 5    \n",
    "#     elif 121 <= days <= 180: \n",
    "#         return 6    \n",
    "#     elif 181 <= days <= 365: \n",
    "#         return 7    \n",
    "#     elif days > 365: \n",
    "#         return 8  \n",
    "#     else:   \n",
    "#         return None  \n",
    "\n",
    "# # Test cases\n",
    "# print(los_to_category(15))  # Expected output: 1\n",
    "# print(los_to_category(35))  # Expected output: 2\n",
    "# print(los_to_category(50))  # Expected output: 3\n",
    "# print(los_to_category(70))  # Expected output: 4\n",
    "# print(los_to_category(100))  # Expected output: 5\n",
    "# print(los_to_category(150))  # Expected output: 6\n",
    "# print(los_to_category(200))  # Expected output: 7\n",
    "# print(los_to_category(400))  # Expected output: 8\n",
    "# print(los_to_category(-5))  # Expected output: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for missing values in the LOS column\n",
    "# missing_values_los = los.isna().sum()\n",
    "# print('Missing values in the LOS column:', missing_values_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for -9 in the LOS column\n",
    "# missing_values_los = los.isin([-9]).sum()\n",
    "# print('Missing values in the LOS column:', missing_values_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = df.copy()\n",
    "# # from conversation with \"house of Hope recovery\" class 4 which is \"transfer to a different facility\" would also be considered a success.\n",
    "# for value in df1['REASON']:\n",
    "#     if value == 1:\n",
    "#         value = value\n",
    "#     elif value == 4:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# df1['REASON'] = df1['REASON'].replace(4, 1)\n",
    "# df1['REASON'] = df1['REASON'].replace(2, 0)\n",
    "# df1['REASON'] = df1['REASON'].replace(3, 0)\n",
    "# df1['REASON'] = df1['REASON'].replace(5, 0)\n",
    "# df1['REASON'] = df1['REASON'].replace(6, 0)\n",
    "# df1['REASON'] = df1['REASON'].replace(7, 0)\n",
    "# df1['REASON'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data starting with SUB1_D \n",
    "# # alcohol could be a success, mmj could be a success\n",
    "# for value in df1['SUB1_D']:\n",
    "#     if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(14, 1)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(16, 1)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(18, 1)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(12, 1)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(13, 1)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(2, 1)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(3, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(4, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(5, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(6, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(7, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(8, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(9, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(10, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(11, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(15, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(17, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(19, 0)\n",
    "\n",
    "# df1['SUB1'] = df1['SUB1'].replace(14, 1)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(16, 1)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(18, 1)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(12, 1)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(13, 1)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(2, 1)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(3, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(4, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(5, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(6, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(7, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(8, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(9, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(10, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(11, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(15, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(17, 0)\n",
    "# df1['SUB1'] = df1['SUB1'].replace(19, 0)\n",
    "\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(14, 1)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(16, 1)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(18, 1)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(12, 1)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(13, 1)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(2, 1)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(3, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(4, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(5, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(6, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(7, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(8, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(9, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(10, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(11, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(15, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(17, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(19, 0)\n",
    "\n",
    "# df1['SUB2'] = df1['SUB2'].replace(14, 1)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(16, 1)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(18, 1)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(12, 1)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(13, 1)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(2, 1)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(3, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(4, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(5, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(6, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(7, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(8, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(9, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(10, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(11, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(15, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(17, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(19, 0)\n",
    "\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(14, 1)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(16, 1)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(18, 1)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(12, 1)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(13, 1)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(2, 1)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(3, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(4, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(5, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(6, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(7, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(8, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(9, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(10, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(11, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(15, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(17, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(19, 0)\n",
    "\n",
    "# df1['SUB3'] = df1['SUB3'].replace(14, 1)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(16, 1)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(18, 1)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(12, 1)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(13, 1)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(2, 1)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(3, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(4, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(5, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(6, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(7, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(8, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(9, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(10, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(11, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(15, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(17, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(19, 0)\n",
    "# # df1['SUB1_D'].notna().value_counts()\n",
    "# # time to clean up the data starting with SUB1 \n",
    "# # alcohol could be a success, mmj could be a success, using the same values for SUB1_D\n",
    "# for value in df1['SUB1']:\n",
    "#     if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# for value in df1['SUB2_D']:\n",
    "#     if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# # df1['SUB1_D'].notna().value_counts()\n",
    "# # time to clean up the data starting with SUB1 \n",
    "# # alcohol could be a success, mmj could be a success, using the same values for SUB1_D\n",
    "# for value in df1['SUB2']:\n",
    "#     if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# for value in df1['SUB3_D']:\n",
    "#     if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# # df1['SUB1_D'].notna().value_counts()\n",
    "# # time to clean up the data starting with SUB1 \n",
    "# # alcohol could be a success, mmj could be a success, using the same values for SUB1_D\n",
    "# for value in df1['SUB3']:\n",
    "#     if value == 1 or value == 14 or value == 16 or value == 18 or value == 12 or value == 13 or value == 2:\n",
    "#         value = 1\n",
    "#     else:\n",
    "#         value = 0\n",
    "# df1['SUB1'] = df1['SUB1'].replace(-9, 0)\n",
    "# df1['SUB1_D'] = df1['SUB1_D'].replace(-9, 0)\n",
    "# df1['SUB2'] = df1['SUB2'].replace(-9, 0)\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 0)\n",
    "# df1['SUB3'] = df1['SUB3'].replace(-9, 0)\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "# print(df1['SUB1'].value_counts())\n",
    "# print(df1['SUB1_D'].value_counts())\n",
    "# print(df1['SUB2'].value_counts())\n",
    "# print(df1['SUB2_D'].value_counts())\n",
    "# print(df1['SUB3'].value_counts())\n",
    "# print(df1['SUB3_D'].value_counts())\n",
    "\n",
    "# # df1['SUB1_D'].notna().value_counts()\n",
    "# # # In column SUB2 replace -9 with 19\n",
    "# df1['SUB2'] = df1['SUB2'].replace(-9, 19)\n",
    "# # print(df1['SUB2'].value_counts())\n",
    "# # # In column SUB2_D replace -9 with 19\n",
    "# # # df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 19)\n",
    "# # print(df1['SUB2_D'].value_counts())\n",
    "# # # In column SUB3 replace -9 with 19\n",
    "# # # df1['SUB3'] = df1['SUB3'].replace(-9, 19)\n",
    "# # print(df1['SUB3'].value_counts())\n",
    "# # # In column SUB3_D replace -9 with 0\n",
    "# # # df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "# # print(df1['SUB3_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with FREQ1_D, FREQ2_D, FREQ3_D\n",
    "# # replacing -9 in the columns with 1 since 1 indicates 'no use' and if data was missing we assumed it was irrelivent to begin with\n",
    "# df1['FREQ1_D'] = df1['FREQ1_D'].replace(-9, 1)\n",
    "# df1['FREQ2_D'] = df1['FREQ2_D'].replace(-9, 1)\n",
    "# df1['FREQ3_D'] = df1['FREQ3_D'].replace(-9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with FREQ1, FREQ2, FREQ3\n",
    "# # replacing -9 in the columns with 1 since 1 indicates 'no use' and if data was missing we assumed it was irrelivent to begin with\n",
    "# df1['FREQ1'] = df1['FREQ1'].replace(-9, 1)\n",
    "# df1['FREQ2'] = df1['FREQ2'].replace(-9, 1)\n",
    "# df1['FREQ3'] = df1['FREQ3'].replace(-9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'CASEID'\n",
    "# df1['CASEID'].notna().value_counts()\n",
    "# # CASEID has no null values and does need info filled\n",
    "# # we can drop this field\n",
    "# df1 = df1.drop(columns=\"CASEID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'AGE'\n",
    "# df1['AGE'].notna().value_counts()\n",
    "# # AGE has no null values and does need info filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'GENDER'\n",
    "# # gender is not a columns where we can fill in the data and not decrease the accuracy. so for all -9 values I am replacing with 0\n",
    "# df1['GENDER'] = df1['GENDER'].replace(-9, 0)\n",
    "# df1['GENDER'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'DISYR'\n",
    "# df1['DISYR'].notna().value_counts()\n",
    "# # DISYR has no null values and does need info filled\n",
    "# # drop this column\n",
    "# df1 = df1.drop(columns=\"DISYR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'RACE'\n",
    "# # for race, '7' indicateds 'other single race  so for all -9 values i replaces them with 7\n",
    "# df1['RACE'] = df1['RACE'].replace(-9, 7)\n",
    "# df1['RACE'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'ETHNIC'\n",
    "# # for ETHNIC, '4' indicateds 'other single race  so for all -9 values I replaced them with 4\n",
    "# df1['ETHNIC'] = df1['ETHNIC'].replace(-9, 4)\n",
    "# df1['ETHNIC'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'MARSTAT'\n",
    "# # for MARSTAT, hard to determine what to fill -9 with so filling with 0 for now\n",
    "# df1['MARSTAT'] = df1['MARSTAT'].replace(-9, 0)\n",
    "# df1['MARSTAT'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'EDUC'\n",
    "# # for EDUC, hard to determine what to fill -9 with. options are 1-5 so filling with 2 as it seems like a fair average\n",
    "# df1['EDUC'] = df1['EDUC'].replace(-9, 2)\n",
    "# df1['EDUC'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'EMPLOY' and 'EMPLOY_D'\n",
    "# # for EMPLOY, hard to determine what to fill -9 with. options are 1-4. filling with 2 for now since if the data is missing is seems like that would indicate 'unemployed'\n",
    "# df1['EMPLOY'] = df1['EMPLOY'].replace(-9, 0)\n",
    "# df1['EMPLOY_D'] = df1['EMPLOY_D'].replace(-9, 0)\n",
    "# df1['EMPLOY'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'DETNLF' and 'DETNLF_D'\n",
    "# # for DETNLF, its just details on the previous columns not in labor force option so im replacing missing values with 0 as to not affect the results as much. we may want to drop this column\n",
    "# # df1['DETNLF'] = df1['DETNLF'].replace(-9, 0)\n",
    "# # df1['DETNLF_D'] = df1['DETNLF_D'].replace(-9, 0)\n",
    "# # df1['DETNLF'].notna().value_counts()\n",
    "# # drop this column\n",
    "# df1 = df1.drop(columns=\"DETNLF\")\n",
    "# df1 = df1.drop(columns=\"DETNLF_D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'PREG'\n",
    "# # for PREG, -9 most likely indicates male patients. replacing with 0 as to not affect the results as much. \n",
    "# df1['PREG'] = df1['PREG'].replace(-9, 2)\n",
    "# df1['PREG'].notna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'VET'\n",
    "# # for VET, -9 most likely indicates not a veteren so im replacing -9 with 2 for 'no'\n",
    "# df1['VET'] = df1['VET'].replace(-9, 2)\n",
    "# df1['VET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'LIVARAG'\n",
    "# # for LIVARAG, its hard to determine what -9 could indicate so im replacing -9 with 0 as to not affect the data as much with the -9's in this column\n",
    "# df1['LIVARAG'] = df1['LIVARAG'].replace(-9, 1)\n",
    "# df1['LIVARAG_D'] = df1['LIVARAG_D'].replace(-9, 1)\n",
    "# df1['LIVARAG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'PRIMINC'\n",
    "# # for PRIMINC, its hard to determine what -9 could indicate so im replacing -9 with 0 as to not affect the data as much with the -9's in this column\n",
    "# df1['PRIMINC'] = df1['PRIMINC'].replace(-9, 4)\n",
    "# df1['PRIMINC'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'ARRESTS' and 'ARRESTS_D'\n",
    "# # for 'ARRESTS' and 'ARRESTS_D', 0 indicates none so im setting -9 to none as that seems most likely to be the case if the information is missing\n",
    "# df1['ARRESTS'] = df1['ARRESTS'].replace(-9, 0)\n",
    "# df1['ARRESTS_D'] = df1['ARRESTS_D'].replace(-9, 0)\n",
    "# df1['ARRESTS'].value_counts()\n",
    "# df1['ARRESTS_D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with ' STFIPS'\n",
    "# # for ' STFIPS', there are no null values in this column but we may want to remove it because the large values could through the accuracy off\n",
    "# df1['STFIPS'].notna().value_counts()\n",
    "# # drop this column\n",
    "# df1 = df1.drop(columns=\"STFIPS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'REGION'\n",
    "# # for 'REGION', there are no null values in this column but we may want to remove it because the values could through the accuracy off and it doesnt seem to be relevant data\n",
    "# df1['REGION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'DIVISION'\n",
    "# # for 'DIVISION', there are no null values in this column but we may want to remove it because the values could through the accuracy off and it doesnt seem to be relevant data\n",
    "# df1['DIVISION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'SERVICES and SERVICES_D'\n",
    "# # for 'SERVICES and SERVICES_D', there are no null values in this column but we may want to remove it because the values could through the accuracy off and it doesnt seem to be relevant data for the questions we are trying to answer\n",
    "# df1['SERVICES'].value_counts()\n",
    "# df1['SERVICES_D'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'DAYWAIT'\n",
    "# # for 'DAYWAIT', its seems safe to replace a value of missing data with a value of '0' to indicate that they didnt wait\n",
    "# df1['DAYWAIT'] = df1['DAYWAIT'].replace(-9, 0)\n",
    "# df1['DAYWAIT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'METHUSE'\n",
    "# # for 'METHUSE', its seems safe to replace a value of missing data with a value of 'none'\n",
    "# df1['METHUSE'] = df1['METHUSE'].replace(-9, 0)\n",
    "# df1['METHUSE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with 'LOS'\n",
    "# # for 'LOS', stands for length of stay this data seems irrelivent to the questions we are trying to answer so I suggest dropping the column to perserve prediction accuracy\n",
    "# # 1-30, 31-45, 46-60, 61-90, 91-120, 121-180, 181-365, more than\n",
    "# def los_to_category(days):\n",
    "#     if days <= 30:\n",
    "#         return 1\n",
    "#     elif days <= 31:\n",
    "#         return 2\n",
    "#     elif days <= 32:\n",
    "#         return 3\n",
    "#     elif days <= 33:\n",
    "#         return 4\n",
    "#     elif days <= 34:\n",
    "#         return 5\n",
    "#     elif days <= 35:\n",
    "#         return 6\n",
    "#     elif days <= 36:\n",
    "#         return 7\n",
    "#     else:\n",
    "#         return 8\n",
    "# # Test cases\n",
    "\n",
    "# # df1['LOS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(los_to_category(15))  # Expected output: 1\n",
    "# print(los_to_category(35))  # Expected output: 2\n",
    "# print(los_to_category(50))  # Expected output: 3\n",
    "# print(los_to_category(70))  # Expected output: 4\n",
    "# print(los_to_category(100))  # Expected output: 5\n",
    "# print(los_to_category(150))  # Expected output: 6\n",
    "# print(los_to_category(200))  # Expected output: 7\n",
    "# print(los_to_category(400))  # Expected output: 8\n",
    "# print(los_to_category(-5))  # Expected output: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1['LOS'] = df1['LOS'].apply(los_to_category)\n",
    "# df1['LOS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with PSOURCE: Referral source\n",
    "# # for 'PSOURCE' this doesnt seem to me to be relevant data so im replacing -9 with 1 for now to indicate that it was self motivated\n",
    "# df1['PSOURCE'] = df1['PSOURCE'].replace(-9, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with DETCRIM: Detailed criminal justice referral\n",
    "# # for 'DETCRIM' im replacing -9 with 0 for now. most of this data is missing. we may want to drop this column\n",
    "# df1['DETCRIM'] = df1['DETCRIM'].replace(-9, 0)\n",
    "# # drop this column\n",
    "# df1 = df1.drop(columns=\"DETCRIM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with NOPRIOR: Previous substance use treatment episodes\n",
    "# # for 'NOPRIOR' im replacing -9 with 0 for now. its could go either way. we will ask clients\n",
    "# df1['NOPRIOR'] = df1['NOPRIOR'].replace(-9, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for 'ROUTE1' im replacing -9 with 5 for now to indicate 'other'. its could go either way. we will ask clients\n",
    "# df1['ROUTE1'] = df1['ROUTE1'].replace(-9, 5)\n",
    "# df1['ROUTE2'] = df1['ROUTE2'].replace(-9, 5)\n",
    "# df1['ROUTE3'] = df1['ROUTE3'].replace(-9, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time to clean up the data with FRSTUSE1: Age at first use (primary)\n",
    "# # FRSTUSE1 is a column where its hard to determine what a value of -9 should be replaced with so im replacing it with 0 for now to have less of an impact on the data\n",
    "# df1['FRSTUSE1'] = df1['FRSTUSE1'].replace(-9, 3)\n",
    "# # adding Beau's code\n",
    "# # In column FRSTUSE2 replace -9 with 0\n",
    "# df1['FRSTUSE2'] = df1['FRSTUSE2'].replace(-9, 3)\n",
    "# print(df1['FRSTUSE2'].value_counts())\n",
    "# # In column FRSTUSE3 replace -9 with 0\n",
    "# df1['FRSTUSE3'] = df1['FRSTUSE3'].replace(-9, 3)\n",
    "# print(df1['FRSTUSE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB2 replace -9 with 19\n",
    "# df1['SUB2'] = df1['SUB2'].replace(-9, 19)\n",
    "# print(df1['SUB2'].value_counts())\n",
    "# # In column SUB2_D replace -9 with 19\n",
    "# df1['SUB2_D'] = df1['SUB2_D'].replace(-9, 19)\n",
    "# print(df1['SUB2_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column SUB3 replace -9 with 19\n",
    "# df1['SUB3'] = df1['SUB3'].replace(-9, 19)\n",
    "# print(df1['SUB3'].value_counts())\n",
    "# # In column SUB3_D replace -9 with 0\n",
    "# df1['SUB3_D'] = df1['SUB3_D'].replace(-9, 0)\n",
    "# print(df1['SUB3_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column ROUTE3 replace -9 with 0\n",
    "# df1['ROUTE3'] = df1['ROUTE3'].replace(-9, 0)\n",
    "# print(df1['ROUTE3'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column DSMCRIT replace -9 with 0\n",
    "# df1['DSMCRIT'] = df1['DSMCRIT'].replace(-9, 5)\n",
    "# print(df1['DSMCRIT'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column PSYPROB replace -9 with 1\n",
    "# df1['PSYPROB'] = df1['PSYPROB'].replace(-9, 1)\n",
    "# print(df1['PSYPROB'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column PRIMPAY replace -9 with 1\n",
    "# df1['PRIMPAY'] = df1['PRIMPAY'].replace(-9, 1)\n",
    "# print(df1['PRIMPAY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # In column FREQ_ATND_SELF_HELP replace -9 with 0\n",
    "# df1['FREQ_ATND_SELF_HELP'] = df1['FREQ_ATND_SELF_HELP'].replace(-9, 3)\n",
    "# print(df1['FREQ_ATND_SELF_HELP'].value_counts())\n",
    "# # In column FREQ_ATND_SELF_HELP_D replace -9 with 0\n",
    "# df1['FREQ_ATND_SELF_HELP_D'] = df1['FREQ_ATND_SELF_HELP_D'].replace(-9, 3)\n",
    "# print(df1['FREQ_ATND_SELF_HELP_D'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reason = df1['REASON']\n",
    "# df1_corr = df1.corr()\n",
    "# df1_corr.unstack().sort_values()\n",
    "# variable = df1_corr['REASON'].sort_values()\n",
    "# variable.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display df1\n",
    "# display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What columns are highly correlated with the target variable 'REASON'?\n",
    "# high_corr_df = df1_corr['REASON'].sort_values()\n",
    "# print('Correlation of REASON with other variables.')\n",
    "# display(high_corr_df.tail(21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize only the positive correlations\n",
    "# positive_correlations = high_corr_df[high_corr_df > 0]\n",
    "# plt.figure(figsize=(20,18))\n",
    "# positive_correlations.plot(kind='bar', color='blue')\n",
    "# plt.title('Visualization of only the positive correlations to the target variable')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor Outputs:\n",
    "\n",
    "* First Run:\n",
    "    * Training Accuracy: 0.9980399157620962\n",
    "    * Testing Accuracy: 0.837835013541325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop the target variable 'REASON' from the dataset\n",
    "# X = df1.drop(columns='REASON')\n",
    "# y = df1['REASON']\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize the StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# # Fit on the training data\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# # Transform the testing data\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Create a Random Forest model\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=4, random_state=42)\n",
    "# et_model = ExtraTreesClassifier(n_estimators=100, max_depth=10, min_samples_split=2, min_samples_leaf=4, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Train the RandomForest model\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions with the RandomForest model\n",
    "# y_train_pred_rf = rf_model.predict(X_train)\n",
    "# y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate the RandomForest accuracies\n",
    "# train_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "# test_accuracy_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "\n",
    "# # Print the RandomForest accuracies\n",
    "# print('RandomForest Training Accuracy:', train_accuracy_rf)\n",
    "# print('RandomForest Testing Accuracy:', test_accuracy_rf)\n",
    "\n",
    "# # Train the ExtraTrees model\n",
    "# et_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions with the ExtraTrees model\n",
    "# y_train_pred_et = et_model.predict(X_train)\n",
    "# y_test_pred_et = et_model.predict(X_test)\n",
    "\n",
    "# # Calculate the ExtraTrees accuracies\n",
    "# train_accuracy_et = accuracy_score(y_train, y_train_pred_et)\n",
    "# test_accuracy_et = accuracy_score(y_test, y_test_pred_et)\n",
    "\n",
    "# # Print the ExtraTrees accuracies\n",
    "# print('ExtraTrees Training Accuracy:', train_accuracy_et)\n",
    "# print('ExtraTrees Testing Accuracy:', test_accuracy_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the RandomForest accuracies\n",
    "# print('RandomForest Training Accuracy:', train_accuracy_rf)\n",
    "# print('RandomForest Testing Accuracy:', test_accuracy_rf)\n",
    "\n",
    "# # Check for overfitting and underfitting in RandomForest\n",
    "# if train_accuracy_rf > test_accuracy_rf:\n",
    "#     print('RandomForest is overfitting.')\n",
    "# elif train_accuracy_rf < test_accuracy_rf:\n",
    "#     print('RandomForest is underfitting.')\n",
    "# else:\n",
    "#     print('RandomForest is fitting well.')\n",
    "\n",
    "# # Print the ExtraTrees accuracies\n",
    "# print('ExtraTrees Training Accuracy:', train_accuracy_et)\n",
    "# print('ExtraTrees Testing Accuracy:', test_accuracy_et)\n",
    "\n",
    "# # Check for overfitting and underfitting in ExtraTrees\n",
    "# if train_accuracy_et > test_accuracy_et:\n",
    "#     print('ExtraTrees is overfitting.')\n",
    "# elif train_accuracy_et < test_accuracy_et:\n",
    "#     print('ExtraTrees is underfitting.')\n",
    "# else:\n",
    "#     print('ExtraTrees is fitting well.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # Perform cross-validation on the RandomForest model\n",
    "# rf_scores = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
    "# print('RandomForest Cross-Validation Accuracy:', np.mean(rf_scores))\n",
    "\n",
    "# # Perform cross-validation on the ExtraTrees model\n",
    "# et_scores = cross_val_score(et_model, X_train, y_train, cv=5)\n",
    "# print('ExtraTrees Cross-Validation Accuracy:', np.mean(et_scores))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
